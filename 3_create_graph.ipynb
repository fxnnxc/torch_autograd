{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch High-order Derivates with create_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x=-2.0,         f = -16.0\n",
      "x=-2.0,     df/dx = 24.0\n",
      "x=-2.0, d^2f/dx^2 = -24.0\n",
      "x=-2.0, d^3f/dx^3 = 12.0\n",
      "x=-2.0, d^4f/dx^4 = 0.0\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "v = -2.0\n",
    "x = torch.tensor(v, requires_grad=True)\n",
    "\n",
    "# function \n",
    "f = 2 * x**3\n",
    "print(f\"x={v},         f = {f}\")\n",
    "\n",
    "# first order\n",
    "f.backward(retain_graph=True)\n",
    "print(f\"x={v},     df/dx = {x.grad}\")\n",
    "\n",
    "# second order\n",
    "x.grad.zero_()\n",
    "df = torch.autograd.grad(f, x, create_graph=True)[0]\n",
    "df.backward(gradient=torch.tensor(1.0))\n",
    "print(f\"x={v}, d^2f/dx^2 = {x.grad}\")\n",
    "\n",
    "# thrid order\n",
    "x.grad.zero_()\n",
    "df = torch.autograd.grad(f, x, create_graph=True)[0]\n",
    "ddf = torch.autograd.grad(df, x, create_graph=True)[0]\n",
    "ddf.backward(gradient=torch.tensor(1.0))\n",
    "print(f\"x={v}, d^3f/dx^3 = {x.grad}\")\n",
    "\n",
    "# fourth order\n",
    "x.grad.zero_()\n",
    "df = torch.autograd.grad(f, x, create_graph=True)[0]\n",
    "ddf = torch.autograd.grad(df, x, create_graph=True)[0]\n",
    "dddf = torch.autograd.grad(ddf, x, create_graph=True)[0]\n",
    "dddf.backward(gradient=torch.tensor(1.0))\n",
    "print(f\"x={v}, d^4f/dx^4 = {x.grad}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the result with the following equations\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\text{(function)~~~} &f = 2x^3 & \\text{when~} x=-2 \\Rightarrow 2 \\cdot (-8) = -16 \\\\\n",
    "    \\text{(first-order)~~~} &\\frac{df}{dx} = 6x^2 &  \\text{when~} x=-2 \\Rightarrow 6 \\cdot 4 = 24 \\\\\n",
    "    \\text{(second-order)~~~} &\\frac{d}{dx}\\Big(\\frac{df}{dx}\\Big) = 12x & \\text{when~} x=-2 \\Rightarrow 12 \\cdot 2 =24 \\\\\n",
    "    \\text{(third-order)~~~} &\\frac{d}{dx}\\Big(\\frac{d^2f}{dx^2}\\Big) = 12 &  \\text{when~} x=-2 \\Rightarrow 12 = 12 \n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Linear(2,5)\n",
    "        self.out = nn.Linear(5, 1)\n",
    "    def forward(self, x):\n",
    "        x = self.mlp(x**2)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.out(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1236, -0.0950])\n",
      "------------\n",
      "tensor([0., 0.])\n",
      "tensor([0., 0.])\n",
      "------------\n",
      "tensor([[0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.2585, 0.2585]], grad_fn=<TBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000],\n",
       "        [0.0000, 0.0000],\n",
       "        [0.0000, 0.0000],\n",
       "        [0.0000, 0.0000],\n",
       "        [0.5169, 0.5169]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net()\n",
    "input= torch.tensor([1.0]*2, requires_grad=True)\n",
    "\n",
    "# first order \n",
    "f = net(input)\n",
    "f.backward()\n",
    "print(input.grad)\n",
    "print(\"------------\")\n",
    "\n",
    "# gradient of weights \n",
    "input.grad.zero_() \n",
    "f = net(input)\n",
    "gx = torch.autograd.grad(f, input, create_graph=True)[0]\n",
    "for i in range(2):\n",
    "    gx[i].backward(retain_graph=True)\n",
    "    print(input.grad)\n",
    "print(\"------------\")\n",
    "\n",
    "# gradient of weights to minimize the norm of the gradients\n",
    "input.grad.zero_() \n",
    "f = net(input)\n",
    "gx = torch.autograd.grad(f, net.mlp.weight, create_graph=True)[0]\n",
    "print(gx)\n",
    "(gx**2).sum().backward()\n",
    "net.mlp.weight.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1. Compute the task specific updated weight \n",
    "$$\n",
    "\\theta' = \\theta - \\alpha \\nabla_\\theta L_{task} (f(\\theta)) \n",
    "$$\n",
    "\n",
    "Step 2. compute the meta-test with the computed weight\n",
    "$$\n",
    "\\theta \\leftarrow \\theta - \\nabla_\\theta L_{{meta}} (f(\\theta'))\n",
    "$$\n",
    "\n",
    "### Implementation \n",
    "\n",
    "1. autograd.grad($L$, $\\theta$, $\\mathrm{\\textcolor{orange}{create\\_graph=True}}$) computes $\\nabla_\\theta L$ as a tensor with computational graph\n",
    "\n",
    "2. forward with weight - grad (obtained by 1) computes the step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Linear(2,5)\n",
    "        self.out = nn.Linear(5, 1)\n",
    "    def forward(self, x, fast_weights=None):\n",
    "        if fast_weights is None:\n",
    "            x = self.mlp(x)\n",
    "            x = nn.functional.relu(x)\n",
    "            x = self.out(x)\n",
    "        else:  # same logit with given weight\n",
    "            w, b = fast_weights[0], fast_weights[1]\n",
    "            x = F.linear(x, w, b)\n",
    "            x = nn.functional.relu(x)\n",
    "            w, b = fast_weights[2], fast_weights[3]\n",
    "            x = F.linear(x, w, b)\n",
    "        return x \n",
    "\n",
    "net = Net()\n",
    "lr=1e-3\n",
    "meta_optim = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "meta_loss = 0\n",
    "num_tasks = 3\n",
    "for task in range(num_tasks):\n",
    "    # Step 1 : compute one_step weight\n",
    "    input= torch.tensor([(task+1.0)]*2)\n",
    "    f = net(input)\n",
    "    grad = torch.autograd.grad(f, net.parameters(), create_graph=True)    # Step 1 Core\n",
    "    fast_weights = list(map(lambda p: p[1] - lr * p[0], zip(grad, net.parameters())))\n",
    "\n",
    "    # Step 2 : compute the loss with one step weight\n",
    "    meta_input= torch.tensor([(task+2.0)]*2)\n",
    "    test_loss = net(input, fast_weights)      # Step 2 Core\n",
    "    meta_loss += test_loss \n",
    "\n",
    "meta_loss /= num_tasks\n",
    "\n",
    "# optimize theta parameters\n",
    "meta_optim.zero_grad()\n",
    "meta_loss.backward()\n",
    "meta_optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hessian \n",
    "\n",
    "Given the Hessian Matrix $H(\\theta)_{N\\times N}$ and vector $r$, \n",
    "\n",
    "the Hessian Vector product is computed by the gradient of the dot product between gradient and the vector. \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "H(\\theta) \\cdot r &= \\frac{d^2}{d\\theta^2} \\Big(  L(\\theta) \\Big) \\cdot r \\\\\n",
    "&= \\frac{d}{d\\theta} \\Big( \\frac{d}{d\\theta} L(\\theta) \\cdot r \\Big)\n",
    "\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "#### Step 1. Compute the gradient \n",
    "\n",
    "#### Step 2. Compute the gradient times vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters : 15\n",
      "Params [Parameter containing:\n",
      "tensor([[ 0.6361,  0.5278],\n",
      "        [ 0.1699,  0.4604],\n",
      "        [-0.0896, -0.2206],\n",
      "        [-0.5090, -0.5044],\n",
      "        [-0.5976, -0.0930]], requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.2209, -0.1456, -0.1973,  0.1386,  0.1548]], requires_grad=True)]\n",
      "----------------------\n",
      "Hessian Vector Product\n",
      "tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]])\n",
      "tensor([[4.7362, 4.7362, 0.0000, 0.0000, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "params = [p for p in net.parameters() if len(p.size()) > 1]\n",
    "N = sum(p.numel() for p in params)\n",
    "print(f\"Number of parameters : {N}\")\n",
    "print(\"Params\", params)\n",
    "\n",
    "# Compute the gardients\n",
    "input= torch.tensor([(task+1.0)]*2)\n",
    "f = net(input)\n",
    "grad = torch.autograd.grad(f, inputs=params, create_graph=True)    # Step 1 Core\n",
    "\n",
    "# Hessian Vector Product\n",
    "prod = torch.autograd.Variable(torch.zeros(1)).type(type(grad[0].data))\n",
    "vec = torch.rand_like(prod)\n",
    "for (g,v) in zip(grad, vec):\n",
    "    prod = prod + (g * v).cpu().sum()  # Step 2 Core\n",
    "    \n",
    "prod.backward()  # Now the params.grad stores the Hessian \n",
    "\n",
    "print(\"----------------------\")\n",
    "print(\"Hessian Vector Product\")\n",
    "for p in params:\n",
    "    print(p.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "728ed223cfa85ea1ef5dcc6c79a939ffd9902707d91f95b40f547e46903ca84f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
